{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeRateDogs Wrangle Report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to wrangle datasets, assess them, clean them, and analyze them through result visualization. The datasets wrangled in this project are in a Twitter page **@dog rates**, also known as **WeRateDogs**. This page rates people's dogs of various breeds by attaching their images to tweets and making humorous comments on each one. This act alone speaks to their unique contents, which, of course, elicits responses from people who commented under each tweet, like the tweet, and retweet it.\n",
    "### This report will aim to give:\n",
    "- a full account on Data Gathering processes\n",
    "- explanations on Data Assessing procedures\n",
    "- account on Data Cleaning processes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling Processes \n",
    "\n",
    "### 1. A full account on Data Gathering processes \n",
    "\n",
    ">For starters, there would be no need for data if the project had not been conceptualized. And there would be no project execution if there was no data. This brings up the steps I took to collect the data required for this wrangling process. The steps taken are as follows:\n",
    ">>  I gathered data for this project through all the three techniques for which data can be gathered.They are, \n",
    ">> - **i. Manually:** **(`twitter archive enhanced.csv`)** was the dataset I manually gathered. This dataset is available on my Udacity Student page. This was obtained by clicking on the **'twitter archive enhanced.csv'** clickable link on my student page to directly download the dataset.\n",
    ">> - **ii. Programmatically:** The second file **(`image-predictions.tsv`)** for this project was gathered programmatically. What I did was to request for the file through url provided by Udacity. To get this file, they ran every image in the WeRateDogs Twitter archive through a neural network that can classify breeds of dogs. After this was done by Udacity instructor, they hosted the dataset on their platform. To assess this file, one need to run **request.get()** with the **url** to the file to able to gather the data. So, with the **url** provided, I ran **request.get(url)** in python library \n",
    ">> - **iii. API:** Since our focus datasets are Twitter data, we can only get the third dataset via **Twitter API.** The first two datasets omitted retweet count and favorite count when gathered. So, to fix up the omissions, I queried **Twitter API** to augment the rest of the datasets. The tweet_id's in the twitter archive came in handy, because I used them to query Twitter API using python's **tweepy** to extract the omitted features and stored each tweet json data as **(`tweet_json.txt`).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explanations on Data Assessing procedures\n",
    "\n",
    "> To assess the issues inherent in each dataset,  I employed python functions. The following were the python functions I used:\n",
    "> - .duplicated()\n",
    "> - .isnull()\n",
    "> - .nunique()\n",
    "> - .info()\n",
    "> - .describe()\n",
    "> - .value_counts()\n",
    "> - .head()\n",
    "> - .tail()\n",
    "> - .testing.assert_series_equal\n",
    "\n",
    "> While evaluating the datasets, I discovered the following **Quality** and **Tidiness** issues in all of the datasets gathered. \n",
    "\n",
    "> #### Quality Issues\n",
    ">> 1. There are retweets in the dataset \n",
    ">> 2. More than 60% of data in `in_reply_to_status_id`, `in_reply_to_user_id`, `retweeted_status_id`, `retweeted_status_user_id` and `retweeted_status_timestamp` column are missing data. \n",
    ">> 3. Tweet_id column not in string datatype \n",
    ">> 4. Img_num feature is not in correct datatype \n",
    ">> 5. Timestamp feature not in datetime datatype\n",
    ">> 6. Incorrect datatype for name column \n",
    ">> 7. There is mixture uppercase and lowercase in p1, p2, and p3 columns\n",
    ">> 8. Names of some dogs are not correct \n",
    ">> 9. Expanded_url column has 59 missing expanded_urls\n",
    "> #### Tidiness Issues\n",
    ">> 1. Separate columns for dog breeds \n",
    ">> 2. All the datasets not in a single dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Account on Data Cleaning processes \n",
    "> By professional standard, cleaning is done on copies of the original dataset. Knowing this guided my first step, which I copied all the datasets before cleaning. I documented each step taken to clean the issues highlighted in Data Assessing in `Define`, `Code`, and `Test`. The following give the summary of techniques used to clean the datasets:\n",
    "> ### Tidiness Issues\n",
    ">> ### Define, Code and Test\n",
    ">> - **doggo**, **floofer**, **pupper**, and **puppo** variables were all collapsed to a single categorical variable as **dog_breed**. There were instances where a dog would have more than one stage, this was coded as **multi_type**. Then .drop() function was to drop all the collapsed variables.   \n",
    ">> - All the datasets were merged together on `tweet_id` to create a single dataset. I used the python function **`merge()`** to combine all the datasets.\n",
    "> ### Quality Issues\n",
    ">> ### Define, Code, and Test\n",
    ">> - Removed all rows that had values (not blank or non-null) in `retweeted_status_id`, `retweeted_status_user_id`, and `retweeted_status_timestamp` columns. This was implemented via **.isnull()** function\n",
    ">> - Dropped `in_reply_to_status_id`, `in_reply_to_user_id`, `retweeted_status_id`, `retweeted_status_user_id` and `retweeted_status_timestamp` columns. They had more than 60% of their data missing\n",
    ">> - Converted tweet_id datatype to string using **`.astype()`**\n",
    ">> - Converted img_num to categorical datatype using **astype()**\n",
    ">> - Removed the timezone from the timestamp column by slicing. Also, converted timestamp column to datetime datatype using **`.to_datetime()`** function\n",
    ">> - Converted name column to categorical datatype using **`.astype('category')`**\n",
    ">> - Converted p1, p2, and p3 columns values to lowercase all through **`.str.lower()`**\n",
    ">> - Dropped the rows with missing data of expanded_urls\n",
    ">> - Since the names of the Dogs were capitalized in the text, any name of dogs that were not capitalized in the name column were categorized as missing value (nan). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the dataset, I stored the dataset as **`twitter_archive_master.csv`** using **`.to_csv`**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
